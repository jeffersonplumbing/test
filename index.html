<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sonar Web App</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
            background-color: #f0f0f0;
        }
        button {
            padding: 15px 30px;
            font-size: 18px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        button:disabled {
            background-color: #6c757d;
        }
        p, h1, h2 {
            font-size: 20px;
            margin: 10px;
        }
        #results {
            text-align: left;
        }
    </style>
</head>
<body>
    <h1>Sonar Web App</h1>
    <p id="status">Ready</p>
    <h2>Results:</h2>
    <div id="results">
        <p id="distance1">Measurement 1: --</p>
        <p id="distance2">Measurement 2: --</p>
        <p id="distance3">Measurement 3: --</p>
        <p id="distance4">Measurement 4: --</p>
        <p id="distance5">Measurement 5: --</p>
    </div>
    <button id="startButton" onclick="startSonar()">Measure Distance to Water</button>

    <script>
        const sampleRate = 44100;
        const startFreq = 200;
        const endFreq = 600;
        const chirpDuration = 0.01;
        const recordDuration = 0.5;
        const chirpGap = 0.5;
        const speedOfSound = 343;
        const metersToFeet = 3.28084;
        let audioContext;
        let mediaRecorder;
        let recordedChunks = [];
        let isPermissionGranted = false;
        let chirpBuffer;

        async function requestMicPermission() {
            console.log('Requesting microphone permission...');
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                isPermissionGranted = true;
                document.getElementById('startButton').disabled = false;
                document.getElementById('status').textContent = 'Permission granted';
                console.log('Microphone permission granted');
                return stream;
            } catch (err) {
                document.getElementById('status').textContent = 'Microphone permission denied';
                document.getElementById('startButton').disabled = true;
                console.error('Permission error:', err.name, err.message);
            }
        }

        function generateChirp() {
            console.log('Generating chirp...');
            const frameCount = Math.floor(chirpDuration * sampleRate);
            const buffer = audioContext.createBuffer(1, frameCount, sampleRate);
            const data = buffer.getChannelData(0);
            const freqRange = endFreq - startFreq;
            for (let i = 0; i < frameCount; i++) {
                const t = i / sampleRate;
                const freq = startFreq + (freqRange * t / chirpDuration);
                data[i] = 0.5 * Math.sin(2 * Math.PI * freq * t);
            }
            return buffer;
        }

        async function processSingleChirp(chirpIndex, stream) {
            return new Promise((resolve) => {
                recordedChunks = [];
                mediaRecorder = new MediaRecorder(stream);
                mediaRecorder.ondataavailable = (e) => {
                    if (e.data.size > 0) recordedChunks.push(e.data);
                    console.log(`Measurement ${chirpIndex} recorded chunk received, size: ${e.data.size}`);
                };

                mediaRecorder.onstop = async () => {
                    console.log(`Processing measurement ${chirpIndex}...`);
                    try {
                        const blob = new Blob(recordedChunks, { type: 'audio/webm' });
                        const arrayBuffer = await blob.arrayBuffer();
                        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                        const recordedData = audioBuffer.getChannelData(0);
                        console.log(`Measurement ${chirpIndex} recorded data length: ${recordedData.length}`);

                        const chirpData = chirpBuffer.getChannelData(0);
                        const correlationLength = recordedData.length + chirpData.length - 1;
                        const correlation = new Float32Array(correlationLength).fill(0);
                        for (let i = 0; i < recordedData.length; i++) {
                            for (let j = 0; j < chirpData.length; j++) {
                                if (i + j < correlationLength) {
                                    correlation[i + j] += recordedData[i] * chirpData[j];
                                }
                            }
                        }

                        const ignoreSamples = Math.floor(chirpDuration * sampleRate) + 100;
                        let maxValue = -Infinity;
                        let maxIndex = 0;
                        for (let i = ignoreSamples; i < correlation.length; i++) {
                            if (correlation[i] > maxValue) {
                                maxValue = correlation[i];
                                maxIndex = i;
                            }
                        }

                        if (maxValue > 0.1) {
                            const rtt = maxIndex / sampleRate;
                            const distanceMeters = (speedOfSound * rtt) / 2;
                            const distanceFeet = distanceMeters * metersToFeet;
                            document.getElementById(`distance${chirpIndex}`).textContent = `Measurement ${chirpIndex}: ${distanceFeet.toFixed(2)} feet`;
                            console.log(`Measurement ${chirpIndex} distance: ${distanceFeet.toFixed(2)} feet`);
                        } else {
                            document.getElementById(`distance${chirpIndex}`).textContent = `Measurement ${chirpIndex}: No echo`;
                            console.log(`Measurement ${chirpIndex}: No echo detected`);
                        }
                    } catch (err) {
                        console.error(`Measurement ${chirpIndex} processing error:`, err);
                        document.getElementById(`distance${chirpIndex}`).textContent = `Measurement ${chirpIndex}: Error`;
                    }
                    resolve();
                };

                mediaRecorder.start();
                const source = audioContext.createBufferSource();
                source.buffer = chirpBuffer;
                source.connect(audioContext.destination);
                source.start();
                console.log(`Playing measurement ${chirpIndex}`);

                setTimeout(() => {
                    mediaRecorder.stop();
                }, recordDuration * 1000);
            });
        }

        async function startSonar() {
            console.log('startSonar called');
            if (!isPermissionGranted) {
                await requestMicPermission();
                if (!isPermissionGranted) return;
            }

            document.getElementById('status').textContent = 'Preparing...';
            document.getElementById('startButton').disabled = true;
            for (let i = 1; i <= 5; i++) {
                document.getElementById(`distance${i}`).textContent = `Measurement ${i}: --`;
            }

            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate });
                console.log('AudioContext created with sample rate:', sampleRate);
                chirpBuffer = generateChirp();
            } catch (err) {
                console.error('AudioContext error:', err);
                document.getElementById('status').textContent = 'AudioContext creation failed';
                document.getElementById('startButton').disabled = false;
                return;
            }

            // 3-second delay before starting chirps
            await new Promise(resolve => setTimeout(resolve, 3000));

            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

            for (let chirpIndex = 1; chirpIndex <= 5; chirpIndex++) {
                document.getElementById('status').textContent = `Active Test (Measurement ${chirpIndex})`;
                await processSingleChirp(chirpIndex, stream);
                if (chirpIndex < 5) {
                    await new Promise(resolve => setTimeout(resolve, chirpGap * 1000));
                }
            }

            stream.getTracks().forEach(track => track.stop());
            document.getElementById('status').textContent = 'Processing complete';
            document.getElementById('startButton').disabled = false;
            audioContext.close();
        }

        // Request permission on page load
        requestMicPermission();
    </script>
</body>
</html>
